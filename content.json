{"meta":{"title":"LKJ Hexo Blog","subtitle":"wow Blog","description":"Blog","author":"LKJ","url":"https://returnwow.github.io"},"pages":[],"posts":[{"title":"Pandas如何去除nan值占百分比大于90的列","slug":"python/questions/Pandas/Pandas_drop_col_nan_percentage","date":"2018-04-29T00:58:04.611Z","updated":"2018-04-29T01:00:13.042Z","comments":true,"path":"2018/04/29/python/questions/Pandas/Pandas_drop_col_nan_percentage/","link":"","permalink":"https://returnwow.github.io/2018/04/29/python/questions/Pandas/Pandas_drop_col_nan_percentage/","excerpt":"","text":"Pandas如何去除nan值占百分比大于90的列 前不久看见有人问如何使用Pandas去除一列中的nan值大于90%的列，然后贴出了未实现的代码，刚刚接触到Pandas，于是比较有兴趣，编复制代码，自己实现了一下。 原问题：请教一个问题：我想把DataFrame中按列读取时缺失值在90%以上的列全部删除，运行后程序没有出错，但是找不到输出文件，我把网上找来的代码改了一下，如下： import pandas as pd data_file = r&#39;E:\\\\123.csv&#39; df = pd.read_csv(data_file) def drop_col(df, cutoff=0.1):# 如果这一列中有90%以上的缺失值，那么就从df中删除这一列 n = len(df) cnt = df.count() if (float(cnt) / n) &lt; cutoff: df.drop(axis=1, inplace=1).to_csv(r&#39;E:\\\\aaa.csv&#39;) 如上代码，首先通过csv读取文件，这个方法问题挺多的，首先代码都没有调用drop_col方法，这样运行完全没有问题，但是完全没有用，本地首先想到的是方法未调用的问题，然后就调用呗，我本地修改的代码不是读取csv，所以下面粘贴我本地的代码： import numpy as np import pandas as pd df = pd.DataFrame({1:[1]*8+[np.nan]*2, 2:[2]*10, 3:[np.nan]*9+[3], 4:[4]*10}) def drop_col(df, cutoff=0.1):#如果这一列中有90%以上的缺失值，那么就从df中删除这一列# n = len(df) cnt = df.count() if (float(cnt) / n) &lt; cutoff: return df.drop(axis=1, inplace=1) df = drop_col(df) print(df) 运行，出错。 Traceback (most recent call last): File &quot;tt.py&quot;, line 22, in &lt;module&gt; df = drop_col(df) File &quot;tt.py&quot;, line 20, in drop_col if (float(cnt) / n) &lt; cutoff: File &quot;C:\\Users\\liukaijiang\\AppData\\Roaming\\Python\\Python36\\site-packages\\panda s\\core\\series.py&quot;, line 112, in wrapper &quot;{0}&quot;.format(str(converter))) TypeError: cannot convert the series to &lt;class &#39;float&#39;&gt; 然后经过一番思考，并修改程序并完成功能。（其中第一版本无法得到所有的满足要求的列） import numpy as np import pandas as pd df = pd.DataFrame({1:[1]*9+[np.nan]*2, 2:[2]*11, 3:[np.nan]*11, 4:[4]*11, 5:[np.nan]*10+[5]}) # 一个包含两列nan值占比大于90%的 print(df) def drop_col(df, cutoff=0.1):#如果这一列中有90%以上的缺失值，那么就从df中删除这一列# n = len(df) cnt = df.count() # 对列进行非nan值计数 cnt = cnt / n # 求出非nan值的百分比 return df.loc[:, cnt[cnt &gt;= cutoff].index] # 根据cnt记录的百分比，过滤出cnt百分百大于等于0.1的（也就是去掉nan值大于0.9的索引），然后对df进行选择，行所有，列为满足要求的cnt的索引。 df = drop_col(df) print(df) 结果显示如下，注意第3（nan值100%）、5（nan值90.9%）列均被选择过滤掉。 &gt;&gt;&gt; df 1 2 3 4 5 0 1.0 2 NaN 4 NaN 1 1.0 2 NaN 4 NaN 2 1.0 2 NaN 4 NaN 3 1.0 2 NaN 4 NaN 4 1.0 2 NaN 4 NaN 5 1.0 2 NaN 4 NaN 6 1.0 2 NaN 4 NaN 7 1.0 2 NaN 4 NaN 8 1.0 2 NaN 4 NaN 9 NaN 2 NaN 4 NaN 10 NaN 2 NaN 4 5.0 &gt;&gt;&gt; drop_col(df) 1 2 4 0 1.0 2 4 1 1.0 2 4 2 1.0 2 4 3 1.0 2 4 4 1.0 2 4 5 1.0 2 4 6 1.0 2 4 7 1.0 2 4 8 1.0 2 4 9 NaN 2 4 10 NaN 2 4 完","categories":[{"name":"Python Question","slug":"Python-Question","permalink":"https://returnwow.github.io/categories/Python-Question/"}],"tags":[{"name":"学习 python Pandas filter","slug":"学习-python-Pandas-filter","permalink":"https://returnwow.github.io/tags/学习-python-Pandas-filter/"}]},{"title":"学习gRPC模块","slug":"python/gRPC/learn_gRPC","date":"2018-03-31T01:38:20.588Z","updated":"2018-03-31T01:39:02.698Z","comments":true,"path":"2018/03/31/python/gRPC/learn_gRPC/","link":"","permalink":"https://returnwow.github.io/2018/03/31/python/gRPC/learn_gRPC/","excerpt":"","text":"gRPC学习笔记 学习来源 开源中国 1、前言 gRPC是一个高性能、开源和通用的RPC框架 面向移动和HTTP/2设计 提供跨平台、跨语言（支持多语言）支持 gRPC基于HTTP/2标准设计，带来双向流、流控、头部压缩、单TCP连接上的多复用请求（连接复用吧）等等，使其在移动设备上表现更好，更省电和节省空间占用。 2、概述 gRPC是语言中立、平台中立、开源的远程过程调用（RPC）系统（比如可以用Java创建服务器，而用Python创建客户端）。2.1、gRPC是什么 与许多 RPC 系统类似，gRPC 也是基于以下理念：定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 gRPC 服务器来处理客户端调用。在客户端拥有一个存根能够像服务端一样的方法。 使得我们可以更容易的创建分布式应用和服务2.2、使用protocol buffers 是一个Google开源的成熟的结构数据序列化机制（如同JSON、XML）。 protocol buffers手册 3、Hello gRPC！ 创建一个官方支持，并是自己熟悉的语言的RPC服务（Hello World），在官方的Github上都存在相应的例子。 准备工作：本机有Git，然后clone需要的版本的源码。 3.1、定义服务 创建一个例子的第一步是定义一个服务（感觉就是protocol buffers的配置文件）：一个RPC服务通过参数和返回类型来指定可以远程调用的方法。 gRPC通过protocol buffers来实现这个功能。 下面将使用protocol buffers接口定义语言来定义服务，使用该协议来定义参数和返回类型。客户端和服务器均使用服务定义生成的接口代码。 syntax = &quot;proto3&quot;; option java_package = &quot;io.grpc.examples&quot;; package helloworld; // The greeter service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user&#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } 3.2、生成gRPC代码 定义好服务之后，就可以使用protocol buffers编译器protoc来创建应用所需的特定客户端和服务器的代码。生成代码的同时包括客户端的存根和服务端要实现的抽象接口，均包含Greeter所定义的方法。 可以使用如下命令生成客户端和服务器端：./run_codegen.sh # shell只能在支持shell的平台 Python编译语法如下，需要安装grpcio-tools，使用pip install grpcio-tools安装，这里边包含编译的工具。 python -m grpc_tools.protoc -I../protos --python_out=. --grpc_python_out=. ../protos/helloworld.proto 其中 -I：为自定的proto文件目录，是一个目录，那么如果我们在这个文件的目录，就选择上级目录，然后定位到protos目录 --python_out &amp; --grpc_python_out：明显是根据proto文件生成的python文件，包含两个文件，暂时不知道具体用处，只知道是有一个是将proto文件定义的rpc绑定到相应的东西上，另外一个相当于做了一个抽象类，里边有proto定义的两个调用的抽象方法，服务器需要实现的方法。 最后的是指定需要编译的proto文件，和前面的-I为组合，试了一下去掉-I参数反之在proto文件上加上相对路径，结果无法编译。 编译好之后，按照官方的服务器和客户端代码，在服务器上实现更新的方法，做相应处理，然后返回，在客户端调用即可。 备注：服务器和客户端必须携带生成的两个代码，这样有点类似分布式实现的Queue了。","categories":[{"name":"Python gRPC","slug":"Python-gRPC","permalink":"https://returnwow.github.io/categories/Python-gRPC/"}],"tags":[{"name":"学习 python gRPC","slug":"学习-python-gRPC","permalink":"https://returnwow.github.io/tags/学习-python-gRPC/"}]},{"title":"学习10分钟入门Pandas模块","slug":"python/10_minutes_to_pandas_note/10_minutes_to_pandas_note","date":"2018-03-31T01:33:32.981Z","updated":"2018-04-26T13:21:43.541Z","comments":true,"path":"2018/03/31/python/10_minutes_to_pandas_note/10_minutes_to_pandas_note/","link":"","permalink":"https://returnwow.github.io/2018/03/31/python/10_minutes_to_pandas_note/10_minutes_to_pandas_note/","excerpt":"","text":"10分钟入门Pandas笔记 引用原文：10分钟入门Pandas 主要引用的包import pandas as pd # pandas库 import numpy as np # numpy库 科学计算，矩阵。 import matplotlib.pyplot as plt # 可视化 画图 一、创建对象 可以通过Data Structure Introduction Setion了解有关本节内容的详细信息。 1、可以通过传递一个list对象来创建一个Series，Pandas会默认创建整型索引：&gt;&gt; s = pd.Series([1,3,5,np.nan,6,8]) &gt;&gt; s 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 2、通过传递一个numpy array，时间索引以及列标签来创建一个DataFrame：&gt;&gt; dates = pd.date_range(&#39;20180329&#39;, periods=6) &gt;&gt; dates DatetimeIndex([&#39;2018-03-29&#39;, &#39;2018-03-30&#39;, &#39;2018-03-31&#39;, &#39;2018-04-01&#39;, &#39;2018-04-02&#39;, &#39;2018-04-03&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) &gt;&gt; df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&#39;ABCD&#39;)) &gt;&gt; df A B C D 2018-03-29 0.916819 0.146020 0.072409 -0.596416 2018-03-30 2.401470 -1.407265 -0.579490 1.653233 2018-03-31 0.415780 0.065466 -1.239546 -0.828074 2018-04-01 -1.917880 -0.470291 -1.102185 0.233711 2018-04-02 0.504557 -2.568238 -0.411014 -0.524040 2018-04-03 0.797517 -1.459728 1.254905 -0.083599 3、通过传递一个能够被转换成类似序列结果的字典对象来创建一个DataFrame：&gt;&gt; df = pd.DataFrame( {&#39;A&#39;:1, &#39;B&#39;:pd.Timestamp(&#39;20180329&#39;), &#39;C&#39;:pd.Series(1, index=list(range(4))), &#39;D&#39;:np.array([3]*4, dtype=&#39;int32&#39;), &#39;E&#39;:pd.Categorical([&#39;test&#39;, &#39;train&#39;, &#39;test&#39;, &#39;train&#39;]), &#39;F&#39;:&#39;foo&#39;}) &gt;&gt; df A B C D E F 0 1 2018-03-29 1 3 test foo 1 1 2018-03-29 1 3 train foo 2 1 2018-03-29 1 3 test foo 3 1 2018-03-29 1 3 train foo 4、查看不同列的数据类型 df.dtypes&gt;&gt; df.dtypes A int64 B datetime64[ns] C int64 D int64 E category F object dtype: object 5、如果使用的是IPython，使用Tab自动补全功能会自动识别所有的数学以及自定义的列。 二、查看数据 详情请参阅：Basics Section1、查看frame中头部和尾部的行&gt;&gt; df A B C D E F 0 1 2018-03-29 1 3 test foo 1 1 2018-03-29 1 3 train foo 2 1 2018-03-29 1 3 test foo 3 1 2018-03-29 1 3 train foo &gt;&gt; df.head(2) A B C D E F 0 1 2018-03-29 1 3 test foo 1 1 2018-03-29 1 3 train foo &gt;&gt; df.tail(2) A B C D E F 2 1 2018-03-29 1 3 test foo 3 1 2018-03-29 1 3 train foo 2、查看索引、列和底层的numpy数据：&gt;&gt; df A B C D E F 0 1 2018-03-29 1 3 test foo 1 1 2018-03-29 1 3 train foo 2 1 2018-03-29 1 3 test foo 3 1 2018-03-29 1 3 train foo &gt;&gt; df.index Int64Index([0, 1, 2, 3], dtype=&#39;int64&#39;) &gt;&gt; df.columns Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;], dtype=&#39;object&#39;) &gt;&gt; df.values array([[1, Timestamp(&#39;2018-03-29 00:00:00&#39;), 1, 3, &#39;test&#39;, &#39;foo&#39;], [1, Timestamp(&#39;2018-03-29 00:00:00&#39;), 1, 3, &#39;train&#39;, &#39;foo&#39;], [1, Timestamp(&#39;2018-03-29 00:00:00&#39;), 1, 3, &#39;test&#39;, &#39;foo&#39;], [1, Timestamp(&#39;2018-03-29 00:00:00&#39;), 1, 3, &#39;train&#39;, &#39;foo&#39;]], dtype=objec t) 3、使用describe()对数据进行快速统计汇总：&gt;&gt; df.describe() A C D count 4.0 4.0 4.0 mean 1.0 1.0 3.0 std 0.0 0.0 0.0 min 1.0 1.0 3.0 25% 1.0 1.0 3.0 50% 1.0 1.0 3.0 75% 1.0 1.0 3.0 max 1.0 1.0 3.0 &gt;&gt; df A B C D E F 0 1 2018-03-29 1 3 test foo 1 1 2018-03-29 1 3 train foo 2 1 2018-03-29 1 3 test foo 3 1 2018-03-29 1 3 train foo &gt;&gt; 4、对数据进行转置操作``` df.T 0 1 2 \\A 1 1 1B 2018-03-29 00:00:00 2018-03-29 00:00:00 2018-03-29 00:00:00C 1 1 1D 3 3 3E test train testF foo foo foo 3 A 1B 2018-03-29 00:00:00C 1D 3E trainF foo &gt; ### 5、按轴进行排序：横轴纵轴是什么？？ df.sort_index(axis=1, ascending=False) # axis = 0 就是列了 D C B A2018-03-29 0.621323 0.232728 -0.245439 1.8876372018-03-30 -0.687133 0.794514 0.462420 -2.2707122018-03-31 -1.453160 -0.512256 0.554050 -1.0682042018-04-01 -1.947354 1.232493 0.656760 0.4307662018-04-02 0.745403 0.529783 0.390616 -0.3498922018-04-03 -0.332525 0.146019 -0.361226 -0.666926``` 6、按值进行排序：&gt;&gt; df.sort_values(&#39;B&#39;) A B C D 2018-04-03 -0.666926 -0.361226 0.146019 -0.332525 2018-03-29 1.887637 -0.245439 0.232728 0.621323 2018-04-02 -0.349892 0.390616 0.529783 0.745403 2018-03-30 -2.270712 0.462420 0.794514 -0.687133 2018-03-31 -1.068204 0.554050 -0.512256 -1.453160 2018-04-01 0.430766 0.656760 1.232493 -1.947354 &gt;&gt; df A B C D 2018-03-29 1.887637 -0.245439 0.232728 0.621323 2018-03-30 -2.270712 0.462420 0.794514 -0.687133 2018-03-31 -1.068204 0.554050 -0.512256 -1.453160 2018-04-01 0.430766 0.656760 1.232493 -1.947354 2018-04-02 -0.349892 0.390616 0.529783 0.745403 2018-04-03 -0.666926 -0.361226 0.146019 -0.332525 三、选择 虽然标准的Python/Numpy的选择和设置表达式都能够直接派上用场，但是作为工程使用的代码，我们推荐使用经过优化的pandas数据访问方式： .at, .iat, .loc, .iloc 和 .ix详情请参阅Indexing and Selecing Data 和 MultiIndex / Advanced Indexing。 1、获取： df[&#39;A&#39;]选择一个单独的列，这将会返回一个Series，等同于df.A&gt;&gt; df[&#39;A&#39;] 2018-03-29 1.887637 2018-03-30 -2.270712 2018-03-31 -1.068204 2018-04-01 0.430766 2018-04-02 -0.349892 2018-04-03 -0.666926 Freq: D, Name: A, dtype: float64 通过[]进行选择，这将会对行进行切片：&gt;&gt; df[&#39;20180329&#39;:&#39;20180331&#39;] A B C D 2018-03-29 1.887637 -0.245439 0.232728 0.621323 2018-03-30 -2.270712 0.462420 0.794514 -0.687133 2018-03-31 -1.068204 0.554050 -0.512256 -1.453160 &gt;&gt; df[0:3] A B C D 2018-03-29 1.887637 -0.245439 0.232728 0.621323 2018-03-30 -2.270712 0.462420 0.794514 -0.687133 2018-03-31 -1.068204 0.554050 -0.512256 -1.453160 2、通过标签选择 使用标签来获取一个交叉的区域：&gt;&gt; df.loc[dates[0]] A 1.887637 B -0.245439 C 0.232728 D 0.621323 Name: 2018-03-29 00:00:00, dtype: float64 &gt;&gt; df.loc[&#39;20180329&#39;] A 1.887637 B -0.245439 C 0.232728 D 0.621323 Name: 2018-03-29 00:00:00, dtype: float64 通过标签来在多个轴上进行选择：&gt;&gt; df.loc[:, [&#39;A&#39;,&#39;C&#39;]] A C 2018-03-29 1.887637 0.232728 2018-03-30 -2.270712 0.794514 2018-03-31 -1.068204 -0.512256 2018-04-01 0.430766 1.232493 2018-04-02 -0.349892 0.529783 2018-04-03 -0.666926 0.146019 标签切片 &gt;&gt;&gt; df.loc[&#39;20180412&#39;:&#39;20180413&#39;, [&#39;A&#39;, &#39;C&#39;]] A C 2018-04-12 1 1 2018-04-13 1 1 对返回的对象进行维度缩减，类似在二维数组中保存一维数组，最终取出一维数组。 &gt;&gt; df.loc[&#39;20180411&#39;, [&#39;A&#39;,&#39;D&#39;]] A 1 D 3 Name: 2018-04-11 00:00:00, dtype: object 获取标量，进一步维度缩减吧。 &gt;&gt; df.loc[&#39;20180411&#39;, &#39;E&#39;] &#39;test&#39; 快速访问一个标量，和上方法相同 &gt;&gt; df.at[&#39;20180411&#39;, &#39;E&#39;] &#39;test&#39; 3、通过位置选择 通过传递数值进行位置选择（选择的是行）, i代表的是index？？？ &gt;&gt; df.iloc[3] # 选择第四行 A 1 B 2018-03-29 00:00:00 C 1 D 3 E train F foo Name: 2018-04-14 00:00:00, dtype: object 通过数值进行切片，与numpy/python中的情况类似 &gt;&gt; df.iloc[1:3, 2:4] # 选择1-2行，2-3列 C D 2018-04-12 1 3 2018-04-13 1 3 对行进行切片 &gt;&gt; df.iloc[1:3, :] # 1-2行，所有列 A B C D E F 2018-04-12 1 2018-03-29 1 3 train foo 2018-04-13 1 2018-03-29 1 3 test foo 对列进行切片 &gt;&gt; df.iloc[:, 2:4] # 全部行，2-3列 C D 2018-04-11 1 3 2018-04-12 1 3 2018-04-13 1 3 2018-04-14 1 3 获取特定的值 &gt;&gt; df.iloc[1,4] # 选择1行的第5个元素 &#39;train&#39; &gt;&gt; df.iat[1,4] # 和上述的loc at iloc对应，带i的是对类似数组的方式访问 &#39;train&#39; 4、布尔索引 使用一个单独列的值来选择数据，感觉是和numpy一样的操作。 &gt;&gt; df A B C D 2018-04-11 -0.859904 0.359774 -0.496582 -0.402911 2018-04-12 -0.025578 0.448901 1.536109 -0.846593 2018-04-13 -0.353849 1.803274 -1.299532 1.344752 2018-04-14 0.396720 -0.248795 0.376469 0.630602 2018-04-15 -0.051567 -0.751154 -0.842248 0.418928 2018-04-16 0.174124 -0.234736 -1.777619 1.005237 &gt;&gt; df[df.A &gt; 0] A B C D 2018-04-14 0.396720 -0.248795 0.376469 0.630602 2018-04-16 0.174124 -0.234736 -1.777619 1.005237 &gt;&gt; df.A &gt; 0 2018-04-11 False 2018-04-12 False 2018-04-13 False 2018-04-14 True 2018-04-15 False 2018-04-16 True Freq: D, Name: A, dtype: bool 使用where操作选择数据: &gt;&gt; df[df &gt; 0] # 取出所有大于零的数据 A B C D 2018-04-11 NaN 0.359774 NaN NaN 2018-04-12 NaN 0.448901 1.536109 NaN 2018-04-13 NaN 1.803274 NaN 1.344752 2018-04-14 0.396720 NaN 0.376469 0.630602 2018-04-15 NaN NaN NaN 0.418928 2018-04-16 0.174124 NaN NaN 1.005237 &gt;&gt; df &gt; 0 A B C D 2018-04-11 False True False False 2018-04-12 False True True False 2018-04-13 False True False True 2018-04-14 True False True True 2018-04-15 False False False True 2018-04-16 True False False True 使用isin()方法来过滤 &gt;&gt; df2 = df.copy() &gt;&gt; df2[&#39;E&#39;] = [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;three&#39;] &gt;&gt; df2 A B C D E 2018-04-11 -0.859904 0.359774 -0.496582 -0.402911 one 2018-04-12 -0.025578 0.448901 1.536109 -0.846593 one 2018-04-13 -0.353849 1.803274 -1.299532 1.344752 two 2018-04-14 0.396720 -0.248795 0.376469 0.630602 three 2018-04-15 -0.051567 -0.751154 -0.842248 0.418928 four 2018-04-16 0.174124 -0.234736 -1.777619 1.005237 three &gt;&gt; df2[&#39;E&#39;].isin([&#39;three&#39;, &#39;two&#39;]) 2018-04-11 False 2018-04-12 False 2018-04-13 True 2018-04-14 True 2018-04-15 False 2018-04-16 True Freq: D, Name: E, dtype: bool &gt;&gt; df2[df2[&#39;E&#39;].isin([&#39;three&#39;, &#39;two&#39;])] A B C D E 2018-04-13 -0.353849 1.803274 -1.299532 1.344752 two 2018-04-14 0.396720 -0.248795 0.376469 0.630602 three 2018-04-16 0.174124 -0.234736 -1.777619 1.005237 three 三、设置 设置一个新的列： &gt;&gt; s1 = pd.Series(range(1,7), index=pd.date_range(&#39;20180411&#39;, periods=6)) &gt;&gt; s1 2018-04-11 1 2018-04-12 2 2018-04-13 3 2018-04-14 4 2018-04-15 5 2018-04-16 6 Freq: D, dtype: int64 &gt;&gt; df[&#39;F&#39;] = s1 &gt;&gt; df A B C D F 2018-04-11 -0.859904 0.359774 -0.496582 -0.402911 1 2018-04-12 -0.025578 0.448901 1.536109 -0.846593 2 2018-04-13 -0.353849 1.803274 -1.299532 1.344752 3 2018-04-14 0.396720 -0.248795 0.376469 0.630602 4 2018-04-15 -0.051567 -0.751154 -0.842248 0.418928 5 2018-04-16 0.174124 -0.234736 -1.777619 1.005237 6 通过标签设置新的值： &gt;&gt; df.at[dates[0], &#39;A&#39;] -0.85990391422115631 &gt;&gt; df.at[dates[0], &#39;A&#39;] = 0 &gt;&gt; df A B C D F 2018-04-11 0.000000 0.359774 -0.496582 -0.402911 1 2018-04-12 -0.025578 0.448901 1.536109 -0.846593 2 2018-04-13 -0.353849 1.803274 -1.299532 1.344752 3 2018-04-14 0.396720 -0.248795 0.376469 0.630602 4 2018-04-15 -0.051567 -0.751154 -0.842248 0.418928 5 2018-04-16 0.174124 -0.234736 -1.777619 1.005237 6 通过位置设置新的值： &gt;&gt; df.iat[0,0] 0.0 &gt;&gt; df.iat[0,0] = 1 &gt;&gt; df A B C D F 2018-04-11 1.000000 0.359774 -0.496582 -0.402911 1 2018-04-12 -0.025578 0.448901 1.536109 -0.846593 2 2018-04-13 -0.353849 1.803274 -1.299532 1.344752 3 2018-04-14 0.396720 -0.248795 0.376469 0.630602 4 2018-04-15 -0.051567 -0.751154 -0.842248 0.418928 5 2018-04-16 0.174124 -0.234736 -1.777619 1.005237 6 通过一个numpy数组设置一组新值，就是将一系列位置的值更改。 &gt;&gt; df.loc[:, &#39;D&#39;] = np.array([5] * len(df)) &gt;&gt; df A B C D F 2018-04-11 1.000000 0.359774 -0.496582 5 1 2018-04-12 -0.025578 0.448901 1.536109 5 2 2018-04-13 -0.353849 1.803274 -1.299532 5 3 2018-04-14 0.396720 -0.248795 0.376469 5 4 2018-04-15 -0.051567 -0.751154 -0.842248 5 5 2018-04-16 0.174124 -0.234736 -1.777619 5 6 通过where操作来设置新的值 &gt;&gt; df2[df2 &gt; 1] A B C D F 2018-04-11 NaN NaN NaN 5 NaN 2018-04-12 NaN NaN 1.536109 5 2.0 2018-04-13 NaN 1.803274 NaN 5 3.0 2018-04-14 NaN NaN NaN 5 4.0 2018-04-15 NaN NaN NaN 5 5.0 2018-04-16 NaN NaN NaN 5 6.0 &gt;&gt; df2[df2 &gt; 1] = -df2 &gt;&gt; df2 A B C D F 2018-04-11 1.000000 0.359774 -0.496582 -5 1 2018-04-12 -0.025578 0.448901 -1.536109 -5 -2 2018-04-13 -0.353849 -1.803274 -1.299532 -5 -3 2018-04-14 0.396720 -0.248795 0.376469 -5 -4 2018-04-15 -0.051567 -0.751154 -0.842248 -5 -5 2018-04-16 0.174124 -0.234736 -1.777619 -5 -6 四、缺失值处理 在pandas中，使用np.nan来代替缺失值，这些值将默认不会包含在计算中，详情请参阅：Missing Data Section 1、reindex()方法可以针对指定轴上的索引进行改变、增加、删除操作，这将返回原始数据的一个拷贝： &gt;&gt; df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [&#39;E&#39;]) &gt;&gt; df1 A B C D F E 2018-04-11 1.000000 0.359774 -0.496582 5 1 NaN 2018-04-12 -0.025578 0.448901 1.536109 5 2 NaN 2018-04-13 -0.353849 1.803274 -1.299532 5 3 NaN 2018-04-14 0.396720 -0.248795 0.376469 5 4 NaN &gt;&gt; df1.loc[dates[0]:dates[1], &#39;E&#39;] = 1 &gt;&gt; df1 A B C D F E 2018-04-11 1.000000 0.359774 -0.496582 5 1 1.0 2018-04-12 -0.025578 0.448901 1.536109 5 2 1.0 2018-04-13 -0.353849 1.803274 -1.299532 5 3 NaN 2018-04-14 0.396720 -0.248795 0.376469 5 4 NaN 去掉包含缺失值的行(NaN)： &gt;&gt; df1.dropna(how=&#39;any&#39;) A B C D F E 2018-04-11 1.000000 0.359774 -0.496582 5 1 1.0 2018-04-12 -0.025578 0.448901 1.536109 5 2 1.0 对缺失值进行填充(NaN)： &gt;&gt; df1.fillna(value=10) A B C D F E 2018-04-11 1.000000 0.359774 -0.496582 5 1 1.0 2018-04-12 -0.025578 0.448901 1.536109 5 2 1.0 2018-04-13 -0.353849 1.803274 -1.299532 5 3 10.0 2018-04-14 0.396720 -0.248795 0.376469 5 4 10.0 对数据进行布尔填充，如位置是NaN，那么就是True： &gt;&gt; pd.isnull(df1) A B C D F E 2018-04-11 False False False False False False 2018-04-12 False False False False False False 2018-04-13 False False False False False True 2018-04-14 False False False False False True 五、相关操作 详情请参阅：Basic Section On Binary Ops统计（相关操作通常情况下不包括NaN值） 1、执行描述性统计： &gt;&gt; df.mean() # 各列求均值 A 0.189975 B 0.229544 C -0.417234 D 5.000000 F 3.500000 dtype: float64 2、在其他轴上进行相同的操作：（行） &gt;&gt; df.mean(1) # 1代表对行进行求均值，0代表对列求均值 2018-04-11 1.372638 2018-04-12 1.791886 2018-04-13 1.629979 2018-04-14 1.904879 2018-04-15 1.671006 2018-04-16 1.832354 Freq: D, dtype: float64 3、对于拥有不同维度，需要对齐的对象进行操作。Pandas会自动的沿着指定的维度进行广播： &gt;&gt; s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2) # shift感觉类似转移数据，移动前面或者后面采用补`NaN`，支持负数。 &gt;&gt; s 2018-04-11 NaN 2018-04-12 NaN 2018-04-13 1.0 2018-04-14 3.0 2018-04-15 5.0 2018-04-16 NaN Freq: D, dtype: float64 &gt;&gt; s 2018-04-11 NaN 2018-04-12 NaN 2018-04-13 1.0 2018-04-14 3.0 2018-04-15 5.0 2018-04-16 NaN Freq: D, dtype: float64 &gt;&gt; df.sub(s, axis=&#39;index&#39;) # sub 做减法 A B C D F 2018-04-11 NaN NaN NaN NaN NaN 2018-04-12 NaN NaN NaN NaN NaN 2018-04-13 -1.353849 0.803274 -2.299532 4.0 2.0 2018-04-14 -2.603280 -3.248795 -2.623531 2.0 1.0 2018-04-15 -5.051567 -5.751154 -5.842248 0.0 0.0 2018-04-16 NaN NaN NaN NaN NaN Apply 1、对数据应用函数：&gt;&gt; df.apply(np.cumsum) # 应用函数，逐行累加 A B C D F 2018-04-11 1.000000 0.359774 -0.496582 5 1 2018-04-12 0.974422 0.808675 1.039527 10 3 2018-04-13 0.620573 2.611949 -0.260005 15 6 2018-04-14 1.017293 2.363153 0.116464 20 10 2018-04-15 0.965726 1.612000 -0.725784 25 15 2018-04-16 1.139850 1.377264 -2.503403 30 21 &gt;&gt; df.apply(lambda x: x.max() - x.min()) # x为接收的一列数据，目的是求极差 A 1.353849 B 2.554428 C 3.313728 D 0.000000 F 5.000000 dtype: float64 直方图，为何叫直方图：实际是统计数据中相同项的个数，然后可以直接画图吧。 具体参考：Histogramming and Discretization&gt;&gt; s = pd.Series(np.random.randint(0,7, size=10)) &gt;&gt; s 0 3 1 6 2 2 3 0 4 0 5 3 6 5 7 0 8 4 9 0 dtype: int32 &gt;&gt; s.value_counts() 0 4 3 2 6 1 5 1 4 1 2 1 dtype: int64 字符串方法Series对象在其str属性中配备了一组字符串处理方法，可以很容易的应用到数组中的每个元素，如下段代码所示。更多详情请参阅：Vectorized String Methods。 &gt;&gt; s = pd.Series([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Aaba&#39;, &#39;Baca&#39;, np.nan, &#39;CABA&#39;, &#39;dog&#39;, &#39;cat&#39;]) &gt;&gt; s.str.lower() 0 a 1 b 2 c 3 aaba 4 baca 5 NaN 6 caba 7 dog 8 cat dtype: object 有如下方法可以使用，基本都是str的方法吧：[&#39;capitalize&#39;,&#39;cat&#39;, &#39;center&#39;, &#39;contains&#39;, &#39;count&#39;, &#39;decode&#39;, &#39;encode&#39;, &#39;endswith&#39;, &#39;extract&#39;,&#39;extractall&#39;, &#39;find&#39;, &#39;findall&#39;, &#39;get&#39;, &#39;get_dummies&#39;, &#39;index&#39;, &#39;isalnum&#39;, &#39;isalpha&#39;, &#39;isdecimal&#39;, &#39;isdigit&#39;, &#39;islower&#39;, &#39;isnumeric&#39;, &#39;isspace&#39;, &#39;istitle&#39;, &#39;isupper&#39;, &#39;join&#39;, &#39;len&#39;, &#39;ljust&#39;, &#39;lower&#39;, &#39;lstrip&#39;, &#39;match&#39;, &#39;normalize&#39;, &#39;pad&#39;,&#39;partition&#39;, &#39;repeat&#39;, &#39;replace&#39;, &#39;rfind&#39;, &#39;rindex&#39;, &#39;rjust&#39;, &#39;rpartition&#39;, &#39;rsplit&#39;, &#39;rstrip&#39;, &#39;slice&#39;, &#39;slice_replace&#39;, &#39;split&#39;, &#39;startswith&#39;, &#39;strip&#39;, &#39;swapcase&#39;, &#39;title&#39;, &#39;translate&#39;, &#39;upper&#39;, &#39;wrap&#39;, &#39;zfill&#39;] 六、合并 Pandas提供了大量的方法能够轻松的对Series，DataFrame和Panel对象就行各种符合这种逻辑关键的合并，具体详阅：Merging section 1.1、Concat，做连接行的操作。 &gt;&gt; df = pd.DataFrame(np.random.randn(10,7)) &gt;&gt; df 0 1 2 3 4 5 6 0 0.553137 -0.461531 -0.178430 0.059118 -0.817795 -0.661965 0.593987 1 1.415022 0.237064 -0.754901 0.654607 1.074064 1.140084 -1.159549 2 -0.605326 -0.410053 -0.020292 -0.727507 -0.294475 0.799932 -0.118478 3 0.954070 0.318788 0.191754 -1.127220 0.294069 -0.245938 -0.402836 4 0.391626 -1.019158 0.759993 -1.401365 0.187595 -0.961535 -2.475896 5 -1.755276 -1.126669 -1.522097 -0.832615 -0.263640 -1.142642 -0.802544 6 0.055710 0.823794 -0.523068 -0.619966 -0.241422 0.894117 -0.011115 7 -0.559937 1.382979 2.352500 -0.500757 -0.601975 2.136011 -1.119064 8 0.631365 0.399121 0.960120 0.667968 1.609791 -0.775155 0.303688 9 1.071855 -0.546073 0.200751 1.845542 0.823959 0.768150 0.935161 &gt;&gt; pieces = [df[:3], df[3:7], df[7:]] &gt;&gt; pd.concat(pieces) 0 1 2 3 4 5 6 0 0.553137 -0.461531 -0.178430 0.059118 -0.817795 -0.661965 0.593987 1 1.415022 0.237064 -0.754901 0.654607 1.074064 1.140084 -1.159549 2 -0.605326 -0.410053 -0.020292 -0.727507 -0.294475 0.799932 -0.118478 3 0.954070 0.318788 0.191754 -1.127220 0.294069 -0.245938 -0.402836 4 0.391626 -1.019158 0.759993 -1.401365 0.187595 -0.961535 -2.475896 5 -1.755276 -1.126669 -1.522097 -0.832615 -0.263640 -1.142642 -0.802544 6 0.055710 0.823794 -0.523068 -0.619966 -0.241422 0.894117 -0.011115 7 -0.559937 1.382979 2.352500 -0.500757 -0.601975 2.136011 -1.119064 8 0.631365 0.399121 0.960120 0.667968 1.609791 -0.775155 0.303688 9 1.071855 -0.546073 0.200751 1.845542 0.823959 0.768150 0.935161 1.2、join 类似于SQL类型的合并，具体请参阅：Database style joining &gt;&gt; left = pd.DataFrame({&#39;key&#39;:[&#39;foo&#39;, &#39;foo&#39;], &#39;lval&#39;:[1,2]}) &gt;&gt; left key lval 0 foo 1 1 foo 2 &gt;&gt; right = pd.DataFrame({&#39;key&#39;:[&#39;foo&#39;, &#39;foo&#39;], &#39;lval&#39;:[4,5]}) &gt;&gt; right key lval 0 foo 4 1 foo 5 &gt;&gt; pd.merge(left, right, on=&#39;key&#39;) key lval_x lval_y 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 &gt;&gt; 1.3、Append将一行连接到一个DataFrame上，具体请参考 Appending: &gt;&gt; df = pd.DataFrame(np.random.randn(8,4), columns=list(&#39;ABCD&#39;)) &gt;&gt; df A B C D 0 0.131836 -1.771433 -0.695097 -0.247473 1 2.501394 0.992452 0.088223 -0.066071 2 -0.041648 0.886106 -1.409708 0.485876 3 2.458939 0.136338 0.727887 0.998972 4 -1.382066 -0.175513 -0.980074 0.586191 5 0.456496 0.061632 2.138576 1.551554 6 -0.134137 1.508475 -0.178692 0.113549 7 0.753561 -0.608606 -0.209655 -0.102295 &gt;&gt; line = df.iloc[3] &gt;&gt; line A 2.458939 B 0.136338 C 0.727887 D 0.998972 Name: 3, dtype: float64 &gt;&gt; df.append(line) A B C D 0 0.131836 -1.771433 -0.695097 -0.247473 1 2.501394 0.992452 0.088223 -0.066071 2 -0.041648 0.886106 -1.409708 0.485876 3 2.458939 0.136338 0.727887 0.998972 4 -1.382066 -0.175513 -0.980074 0.586191 5 0.456496 0.061632 2.138576 1.551554 6 -0.134137 1.508475 -0.178692 0.113549 7 0.753561 -0.608606 -0.209655 -0.102295 3 2.458939 0.136338 0.727887 0.998972 &gt;&gt; df.append(line, ignore_index=True) A B C D 0 0.131836 -1.771433 -0.695097 -0.247473 1 2.501394 0.992452 0.088223 -0.066071 2 -0.041648 0.886106 -1.409708 0.485876 3 2.458939 0.136338 0.727887 0.998972 4 -1.382066 -0.175513 -0.980074 0.586191 5 0.456496 0.061632 2.138576 1.551554 6 -0.134137 1.508475 -0.178692 0.113549 7 0.753561 -0.608606 -0.209655 -0.102295 8 2.458939 0.136338 0.727887 0.998972 七、分组 对于group by操作，我们通常是指以下一个或多个操作步骤：1、(Splitting) 按照一些规则将数据分为不同的组；2、(Applying) 对于每组数据分别执行一个函数；3、(Combining) 将结果组合到一个数据结构中；详情请参阅：Grouping section&gt;&gt; df = pd.DataFrame({&#39;A&#39; : [&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;,&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;foo&#39;], &#39;B&#39; : [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;two&#39;, &#39;two&#39;, &#39;one&#39;, &#39;three&#39;], &#39;C&#39; :np.random.randn(8), &#39;D&#39; : np.random.randn(8)}) &gt;&gt; df A B C D 0 foo one -0.118332 -0.833735 1 bar one 0.359787 1.333599 2 foo two 0.961724 -1.966801 3 bar three -3.627354 -0.587706 4 foo two -1.281741 -0.403980 5 bar two -1.390664 -1.483122 6 foo one 1.403112 -0.213065 7 foo three 0.399807 0.929839 1、分组并对每个分组执行sum函数&gt;&gt; df.groupby(&#39;A&#39;).sum() # 类似分类汇总吧 C D A bar -4.658232 -0.737229 foo 1.364570 -2.487742 2、对多个列进行分组形成一个层次索引，然后执行函数：&gt;&gt; df.groupby([&#39;A&#39;, &#39;B&#39;]).sum() C D A B bar one 0.359787 1.333599 three -3.627354 -0.587706 two -1.390664 -1.483122 foo one 1.284780 -1.046800 three 0.399807 0.929839 two -0.320017 -2.370781 &gt;&gt; 八、Reshaping 详情请参阅：Hierarchical Indexing 和 Reshaping 1、Stack&gt;&gt; tuples = list(zip(*[[&#39;bar&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;baz&#39;, &#39;foo&#39;, &#39;foo&#39;, &#39;quz&#39;, &#39;quz&#39;] ,[&#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;, &#39;one&#39;, &#39;two&#39;]])) &gt;&gt; tuples [(&#39;bar&#39;, &#39;one&#39;), (&#39;bar&#39;, &#39;two&#39;), (&#39;baz&#39;, &#39;one&#39;), (&#39;baz&#39;, &#39;two&#39;), (&#39;foo&#39;, &#39;one&#39;), (&#39;foo&#39;, &#39;two&#39;), (&#39;quz&#39;, &#39;one&#39;), (&#39;quz&#39;, &#39;two&#39;)] &gt;&gt; index = pd.MultiIndex.from_tuples(tuples, names=[&#39;first&#39;, &#39;second&#39;]) &gt;&gt; index MultiIndex(levels=[[&#39;bar&#39;, &#39;baz&#39;, &#39;foo&#39;, &#39;quz&#39;], [&#39;one&#39;, &#39;two&#39;]], labels=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]], names=[&#39;first&#39;, &#39;second&#39;]) &gt;&gt; df = pd.DataFrame(np.random.randn(8,2), index=index, columns=[&#39;A&#39;, &#39;B&#39;]) &gt;&gt; df A B first second bar one 0.192085 1.408195 two 1.866444 0.635571 baz one 0.812173 -0.442293 two -0.969346 0.697064 foo one -1.039750 -0.167861 two 0.359010 0.795804 quz one 0.315553 0.591019 two 0.177442 -1.071087 &gt;&gt; df2 = df[:4] &gt;&gt; df2 A B first second bar one 0.192085 1.408195 two 1.866444 0.635571 baz one 0.812173 -0.442293 two -0.969346 0.697064 &gt;&gt; stacked = df2.stack() &gt;&gt; stacked first second bar one A 0.192085 B 1.408195 two A 1.866444 B 0.635571 baz one A 0.812173 B -0.442293 two A -0.969346 B 0.697064 dtype: float64 &gt;&gt; stacked.unstack() A B first second bar one 0.192085 1.408195 two 1.866444 0.635571 baz one 0.812173 -0.442293 two -0.969346 0.697064 &gt;&gt; stacked.unstack(1) second one two first bar A 0.192085 1.866444 B 1.408195 0.635571 baz A 0.812173 -0.969346 B -0.442293 0.697064 &gt;&gt; stacked.unstack(0) first bar baz second one A 0.192085 0.812173 B 1.408195 -0.442293 two A 1.866444 -0.969346 B 0.635571 0.697064 2、数据透视表，详情请参阅：Pivot Tables&gt;&gt; df = pd.DataFrame({&#39;A&#39; : [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;]*3, &#39;B&#39;:[&#39;A&#39;, &#39;B&#39;, &#39;C &#39;] * 4, &#39;C&#39;:[&#39;foo&#39;, &#39;foo&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;bar&#39;, &#39;bar&#39;]*2, &#39;D&#39;:np.random.randn(12 ), &#39;E&#39;:np.random.randn(12)}) &gt;&gt; df A B C D E 0 one A foo -2.217020 -0.706020 1 one B foo -0.680808 -1.987900 2 two C foo -1.744418 -0.037439 3 three A bar 1.010820 -0.526640 4 one B bar -0.820246 -0.601110 5 one C bar 1.348459 0.286765 6 two A foo 1.497069 1.148904 7 three B foo -1.284661 1.134721 8 one C foo 0.449730 0.487581 9 one A bar -0.601461 0.177472 10 two B bar -2.329131 -1.519148 11 three C bar 0.058038 -0.217201 &gt;&gt; pd.pivot_table(df, values=&#39;D&#39;, index=[&#39;A&#39;, &#39;B&#39;], columns=&#39;C&#39;) C bar foo A B one A -0.601461 -2.217020 B -0.820246 -0.680808 C 1.348459 0.449730 three A 1.010820 NaN B NaN -1.284661 C 0.058038 NaN two A NaN 1.497069 B -2.329131 NaN C NaN -1.744418 &gt;&gt; 九、时间序列 Pandas在对频率转换进行重新采样的时候拥有非常简单、强大且高效的功能（如将按秒采样的数据转换为按5分钟为单位进行采样的数据）。这种操作在金融领域很常见。具体参考：Time Series section&gt;&gt; rng = pd.date_range(&#39;4/17/2018&#39;, periods=100, freq=&#39;S&#39;) &gt;&gt; ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) &gt;&gt; ts.resample(&#39;5Min&#39;, how=&#39;sum&#39;) __main__:1: FutureWarning: how in .resample() is deprecated # 最新版本已经被遗弃了 the new syntax is .resample(...).sum() # 推荐使用这种方式。 2018-04-17 25671 Freq: 5T, dtype: int32 &gt;&gt; ts.resample(&#39;5Min&#39;).sum() 2018-04-17 25671 Freq: 5T, dtype: int32 1、时区表示&gt;&gt; rng = pd.date_range(&#39;4/17/2018&#39;, periods=5, freq=&#39;D&#39;) &gt;&gt; rng DatetimeIndex([&#39;2018-04-17&#39;, &#39;2018-04-18&#39;, &#39;2018-04-19&#39;, &#39;2018-04-20&#39;, &#39;2018-04-21&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) &gt;&gt; ts = pd.Series(np.random.randn(len(rng)), rng) &gt;&gt; ts 2018-04-17 -0.994141 2018-04-18 0.713261 2018-04-19 0.956092 2018-04-20 -0.186506 2018-04-21 0.565844 Freq: D, dtype: float64 &gt;&gt; ts_utc = ts.tz_localize(&#39;UTC&#39;) &gt;&gt; ts_utc 2018-04-17 00:00:00+00:00 -0.994141 2018-04-18 00:00:00+00:00 0.713261 2018-04-19 00:00:00+00:00 0.956092 2018-04-20 00:00:00+00:00 -0.186506 2018-04-21 00:00:00+00:00 0.565844 Freq: D, dtype: float64 2、时区转换&gt;&gt; ts_utc.tz_convert(&#39;US/Eastern&#39;) # 北京时间是什么关键字？ 2018-04-16 20:00:00-04:00 -0.994141 2018-04-17 20:00:00-04:00 0.713261 2018-04-18 20:00:00-04:00 0.956092 2018-04-19 20:00:00-04:00 -0.186506 2018-04-20 20:00:00-04:00 0.565844 Freq: D, dtype: float64 &gt;&gt; 时间跨度转换&gt;&gt; rng = pd.date_range(&#39;4/17/2018&#39;, periods=5, freq=&#39;M&#39;) &gt;&gt; rng DatetimeIndex([&#39;2018-04-30&#39;, &#39;2018-05-31&#39;, &#39;2018-06-30&#39;, &#39;2018-07-31&#39;, &#39;2018-08-31&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;M&#39;) &gt;&gt; ts = pd.Series(np.random.randn(len(rng)), rng) &gt;&gt; ts 2018-04-30 3.007299 2018-05-31 2.438724 2018-06-30 -0.063751 2018-07-31 -0.825316 2018-08-31 0.127897 Freq: M, dtype: float64 &gt;&gt; ps = ts.to_period() &gt;&gt; ps 2018-04 3.007299 2018-05 2.438724 2018-06 -0.063751 2018-07 -0.825316 2018-08 0.127897 Freq: M, dtype: float64 &gt;&gt; ps.to_timestamp() 2018-04-01 3.007299 2018-05-01 2.438724 2018-06-01 -0.063751 2018-07-01 -0.825316 2018-08-01 0.127897 Freq: MS, dtype: float64 4、时间和时间戳之间的转换使得可以使用一些方便的算术函数。&gt;&gt; prng = pd.period_range(&#39;2018Q1&#39;, &#39;2028Q4&#39;, freq=&#39;Q-NOV&#39;) &gt;&gt; prng PeriodIndex([&#39;2018Q1&#39;, &#39;2018Q2&#39;, &#39;2018Q3&#39;, &#39;2018Q4&#39;, &#39;2019Q1&#39;, &#39;2019Q2&#39;, &#39;2019Q3&#39;, &#39;2019Q4&#39;, &#39;2020Q1&#39;, &#39;2020Q2&#39;, &#39;2020Q3&#39;, &#39;2020Q4&#39;, &#39;2021Q1&#39;, &#39;2021Q2&#39;, &#39;2021Q3&#39;, &#39;2021Q4&#39;, &#39;2022Q1&#39;, &#39;2022Q2&#39;, &#39;2022Q3&#39;, &#39;2022Q4&#39;, &#39;2023Q1&#39;, &#39;2023Q2&#39;, &#39;2023Q3&#39;, &#39;2023Q4&#39;, &#39;2024Q1&#39;, &#39;2024Q2&#39;, &#39;2024Q3&#39;, &#39;2024Q4&#39;, &#39;2025Q1&#39;, &#39;2025Q2&#39;, &#39;2025Q3&#39;, &#39;2025Q4&#39;, &#39;2026Q1&#39;, &#39;2026Q2&#39;, &#39;2026Q3&#39;, &#39;2026Q4&#39;, &#39;2027Q1&#39;, &#39;2027Q2&#39;, &#39;2027Q3&#39;, &#39;2027Q4&#39;, &#39;2028Q1&#39;, &#39;2028Q2&#39;, &#39;2028Q3&#39;, &#39;2028Q4&#39;], dtype=&#39;period[Q-NOV]&#39;, freq=&#39;Q-NOV&#39;) &gt;&gt; ts = pd.Series(np.random.randn(len(prng)), prng) &gt;&gt; ts.index = (prng.asfreq(&#39;M&#39;, &#39;e&#39;) + 1).asfreq(&#39;H&#39;, &#39;s&#39;) + 9 &gt;&gt; ts.head() 2018-03-01 09:00 0.499979 2018-06-01 09:00 -2.246589 2018-09-01 09:00 -0.824984 2018-12-01 09:00 -0.578511 2019-03-01 09:00 -0.310868 Freq: H, dtype: float64 十、Categorical 从0.15版本开始，pandas可以在DataFrame中支持Categorical类型的数据，详细请看：categorical introduction 和 API documentation&gt;&gt;&gt; df = pd.DataFrame({&#39;id&#39;: [1,2,3,4,5,6], &#39;raw_grade&#39;:[&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;e&#39;]}) 将原始的grade转换为Categorical数据类型&gt;&gt; df[&#39;grade&#39;] = df[&#39;raw_grade&#39;].astype(&#39;category&#39;) &gt;&gt; df[&#39;grade&#39;] 0 a 1 b 2 b 3 a 4 a 5 e Name: grade, dtype: category Categories (3, object): [a, b, e] 将Categorical类型数据命名为更有意义的名称： &gt;&gt; df[&#39;grade&#39;].cat.categories = [&#39;very good&#39;, &#39;good&#39;, &#39;very bad&#39;] &gt;&gt; df id raw_grade grade 0 1 a very good 1 2 b good 2 3 b good 3 4 a very good 4 5 a very good 5 6 e very bad 对类别进行重新排序，并增加完整的类别： &gt;&gt; df[&#39;grade&#39;] = df[&#39;grade&#39;].cat.set_categories([&#39;very bad&#39;, &#39;bad&#39;, &#39;medium&#39;, &#39;good&#39;, &#39;very good&#39;]) &gt;&gt; df[&#39;grade&#39;] 0 very good 1 good 2 good 3 very good 4 very good 5 very bad Name: grade, dtype: category Categories (5, object): [very bad, bad, medium, good, very good] 排序按照的是Categorical的顺序进行的而不是按照字典顺序进行： &gt;&gt; df.sort_values(&#39;grade&#39;) id raw_grade grade 5 6 e very bad 1 2 b good 2 3 b good 0 1 a very good 3 4 a very good 4 5 a very good 对Categorical列进行排序时存在空的类别，目前最新版本是0，而非NaN。 &gt;&gt; df.groupby(&#39;grade&#39;).size() grade very bad 1 bad 0 medium 0 good 2 very good 3 dtype: int64 十一、画图 具体看文档：Plotting docs&gt;&gt; ts = pd.Series(np.random.randn(1000), index=pd.date_range(&#39;20180417&#39;, periods=1000)) &gt;&gt; ts.plot().get_figure().savefig(&#39;ts.png&#39;) 对于DataFrame来说，plot是一种将所有列及其标签进行绘制的简便方法：&gt;&gt; df = pd.DataFrame(np.random.randn(1000, 4), index = ts.index, columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) &gt;&gt; df = df.cumsum() &gt;&gt; df.plot().get_figure().savefig(&#39;df.png&#39;) 十二、导入和保存数据 1、csv，参考：Writing to a csv file 1.1、写入文件：&gt;&gt;&gt; df.to_csv(&#39;foo.csv&#39;) 1.2、从csv读取：&gt;&gt;&gt; pd.read_csv(&#39;foo.csv&#39;) 下面就不用说了，HDF5、Excel基本都是一样，主要考虑的是格式问题，其中读写CSV的时候遇到，单独给某一列数据前面加UFT-8 无BOM格式的前缀，暂时没能找到什么方法解决，\\xef\\xbb\\xbf这个前缀，需要在读取的时候自动匹配去掉。","categories":[{"name":"Python","slug":"Python","permalink":"https://returnwow.github.io/categories/Python/"}],"tags":[{"name":"学习 python Pandas 10minutes","slug":"学习-python-Pandas-10minutes","permalink":"https://returnwow.github.io/tags/学习-python-Pandas-10minutes/"}]},{"title":"Python进制问题","slug":"python/common/hex&octal_problem","date":"2018-01-27T00:44:05.672Z","updated":"2018-01-27T02:20:31.536Z","comments":true,"path":"2018/01/27/python/common/hex&octal_problem/","link":"","permalink":"https://returnwow.github.io/2018/01/27/python/common/hex&octal_problem/","excerpt":"","text":"16进制和8进制的问题 今天在网上看到有人在问\\x20（16进制）和\\20（8进制）的问题 &gt;&gt; &#39;\\x20&#39; &#39; &#39; &gt;&gt; &#39;\\20&#39; &#39;\\x10&#39; 如上，因为这个存在特殊性：\\x20对应是10进制是32，而32Ascii码对应的控制字符空格Space，这个一眼就看出来了是16进制，而且也知道是16进制。然而\\20就不好解释了，平时接触8进制也较少，刚开始以为是自动转换16进制，但是打印的结果显然推翻了结论，后续还想到\\是转义字符，那么\\2等有特殊意义。于是开始从\\1打印到\\10，如下所示： &gt;&gt; &#39;\\7&#39; &#39;\\x07&#39; &gt;&gt; &#39;\\8&#39; &#39;\\\\8&#39; &gt;&gt; &#39;\\9&#39; &#39;\\\\9&#39; &gt;&gt; &#39;\\10&#39; &#39;\\x08&#39; 发现\\8打印很奇怪，\\10打印的16进制是\\x08，然后才想到是8进制，因为8进制表示10进制8需要进位，所以8进制的8就是\\10。 ASCII码表 来源百度百科 Bin(二进制) Oct(八进制) Dec(十进制) Hex(十六进制) 缩写/字符 解释 0000 0000 0 0 0 NUL(null) 空字符 0000 0001 1 1 1 SOH(start of headline) 标题开始 0000 0010 2 2 2 STX (start of text) 正文开始 0000 0011 3 3 3 ETX (end of text) 正文结束 0000 0100 4 4 4 EOT (end of transmission) 传输结束 0000 0101 5 5 5 ENQ (enquiry) 请求 0000 0110 6 6 6 ACK (acknowledge) 收到通知 0000 0111 7 7 7 BEL (bell) 响铃 0000 1000 10 8 8 BS (backspace) 退格 0000 1001 11 9 9 HT (horizontal tab) 水平制表符 0000 1010 12 10 0A “LF (NL line feed new line)” 换行键 0000 1011 13 11 0B VT (vertical tab) 垂直制表符 0000 1100 14 12 0C “FF (NP form feed new page)” 换页键 0000 1101 15 13 0D CR (carriage return) 回车键 0000 1110 16 14 0E SO (shift out) 不用切换 0000 1111 17 15 0F SI (shift in) 启用切换 0001 0000 20 16 10 DLE (data link escape) 数据链路转义 0001 0001 21 17 11 DC1 (device control 1) 设备控制1 0001 0010 22 18 12 DC2 (device control 2) 设备控制2 0001 0011 23 19 13 DC3 (device control 3) 设备控制3 0001 0100 24 20 14 DC4 (device control 4) 设备控制4 0001 0101 25 21 15 NAK (negative acknowledge) 拒绝接收 0001 0110 26 22 16 SYN (synchronous idle) 同步空闲 0001 0111 27 23 17 ETB (end of trans. block) 结束传输块 0001 1000 30 24 18 CAN (cancel) 取消 0001 1001 31 25 19 EM (end of medium) 媒介结束 0001 1010 32 26 1A SUB (substitute) 代替 0001 1011 33 27 1B ESC (escape) 换码(溢出) 0001 1100 34 28 1C FS (file separator) 文件分隔符 0001 1101 35 29 1D GS (group separator) 分组符 0001 1110 36 30 1E RS (record separator) 记录分隔符 0001 1111 37 31 1F US (unit separator) 单元分隔符 0010 0000 40 32 20 (space) 空格 0010 0001 41 33 21 ! 叹号 0010 0010 42 34 22 “””” 双引号 0010 0011 43 35 23 # 井号 0010 0100 44 36 24 $ 美元符 0010 0101 45 37 25 % 百分号 0010 0110 46 38 26 &amp; 和号 0010 0111 47 39 27 ‘ 闭单引号 0010 1000 50 40 28 ( 开括号 0010 1001 51 41 29 ) 闭括号 0010 1010 52 42 2A * 星号 0010 1011 53 43 2B + 加号 0010 1100 54 44 2C “,” 逗号 0010 1101 55 45 2D - 减号/破折号 0010 1110 56 46 2E . 句号 101111 57 47 2F / 斜杠 110000 60 48 30 0 数字0 110001 61 49 31 1 数字1 110010 62 50 32 2 数字2 110011 63 51 33 3 数字3 110100 64 52 34 4 数字4 110101 65 53 35 5 数字5 110110 66 54 36 6 数字6 110111 67 55 37 7 数字7 111000 70 56 38 8 数字8 111001 71 57 39 9 数字9 111010 72 58 3A : 冒号 111011 73 59 3B ; 分号 111100 74 60 3C &lt; 小于 111101 75 61 3D = 等号 111110 76 62 3E &gt; 大于 111111 77 63 3F ? 问号 1000000 100 64 40 @ 电子邮件符号 1000001 101 65 41 A 大写字母A 1000010 102 66 42 B 大写字母B 1000011 103 67 43 C 大写字母C 1000100 104 68 44 D 大写字母D 1000101 105 69 45 E 大写字母E 1000110 106 70 46 F 大写字母F 1000111 107 71 47 G 大写字母G 1001000 110 72 48 H 大写字母H 1001001 111 73 49 I 大写字母I 1001010 112 74 4A J 大写字母J 1001011 113 75 4B K 大写字母K 1001100 114 76 4C L 大写字母L 1001101 115 77 4D M 大写字母M 1001110 116 78 4E N 大写字母N 1001111 117 79 4F O 大写字母O 1010000 120 80 50 P 大写字母P 1010001 121 81 51 Q 大写字母Q 1010010 122 82 52 R 大写字母R 1010011 123 83 53 S 大写字母S 1010100 124 84 54 T 大写字母T 1010101 125 85 55 U 大写字母U 1010110 126 86 56 V 大写字母V 1010111 127 87 57 W 大写字母W 1011000 130 88 58 X 大写字母X 1011001 131 89 59 Y 大写字母Y 1011010 132 90 5A Z 大写字母Z 1011011 133 91 5B [ 开方括号 1011100 134 92 5C \\ 反斜杠 1011101 135 93 5D ] 闭方括号 1011110 136 94 5E ^ 脱字符 1011111 137 95 5F _ 下划线 1100000 140 96 60 ` 开单引号 1100001 141 97 61 a 小写字母a 1100010 142 98 62 b 小写字母b 1100011 143 99 63 c 小写字母c 1100100 144 100 64 d 小写字母d 1100101 145 101 65 e 小写字母e 1100110 146 102 66 f 小写字母f 1100111 147 103 67 g 小写字母g 1101000 150 104 68 h 小写字母h 1101001 151 105 69 i 小写字母i 1101010 152 106 6A j 小写字母j 1101011 153 107 6B k 小写字母k 1101100 154 108 6C l 小写字母l 1101101 155 109 6D m 小写字母m 1101110 156 110 6E n 小写字母n 1101111 157 111 6F o 小写字母o 1110000 160 112 70 p 小写字母p 1110001 161 113 71 q 小写字母q 1110010 162 114 72 r 小写字母r 1110011 163 115 73 s 小写字母s 1110100 164 116 74 t 小写字母t 1110101 165 117 75 u 小写字母u 1110110 166 118 76 v 小写字母v 1110111 167 119 77 w 小写字母w 1111000 170 120 78 x 小写字母x 1111001 171 121 79 y 小写字母y 1111010 172 122 7A z 小写字母z 1111011 173 123 7B { 开花括号 1111100 174 124 7C &#124; 垂线 1111101 175 125 7D } 闭花括号 1111110 176 126 7E ~ 波浪号 1111111 177 127 7F DEL (delete) 删除","categories":[{"name":"Python","slug":"Python","permalink":"https://returnwow.github.io/categories/Python/"}],"tags":[{"name":"学习 Python Hex Octal","slug":"学习-Python-Hex-Octal","permalink":"https://returnwow.github.io/tags/学习-Python-Hex-Octal/"}]},{"title":"学习bs4模块","slug":"python/bs4/learn_bs4","date":"2017-11-29T12:14:07.490Z","updated":"2018-03-31T01:36:17.510Z","comments":true,"path":"2017/11/29/python/bs4/learn_bs4/","link":"","permalink":"https://returnwow.github.io/2017/11/29/python/bs4/learn_bs4/","excerpt":"","text":"学习bs4模块 直接在官方网站上面学习 介绍 bs4是一个格式化读取xml，html文档的库可以使xml、html按照标签的形式操作，具体是将HTML文档转换成一个复杂的树形结构。官方推荐lxml来解析HTML所有节点对象可以归纳为4种: Tag , NavigableString , BeautifulSoup , Comment . 简单使用操作 from bs4 import BeautifulSoup html_doc = &#39;&#39; soup = BeautifulSoup(html_doc, &#39;html.parser&#39;) print(soup.prettify()) #美化输出，格式化缩进。 一般属性 soup.title：获取页面的标题，包含标签 soup.title.name：获取页面标题标签的内容 soup.title.string：获取页面标题的内容 soup.title.parent.name：父标签名字 soup.p：p是标签名字 soup.p[&#39;class&#39;]：第一个匹配p的标签，成员class soup.a：同上 一般方法 soup.find_all(&#39;a&#39;)：找到所以的a标签 查找所有的a标签for link in soup.find_all(&#39;a&#39;): print(link.get(&#39;href&#39;)) # http://example.com/elsie # http://example.com/lacie # http://example.com/tillie soup.get_text()：拿到文档中所有的文字内容，实际测试新浪的首页存在问题，使用html5lib也一致。 对象的种类Tag Tag对象和XML或HTML原生文档中的tag相同 Tag重要的属性 name每个Tag都有自己的名字，通过name属性来获取，比如xxx，Tag名称就是title如果改变了某个tag的名称，那么将影响当前生成的hml对象文档tag.name = &#39;xxx&#39; Attributes一个Tag可能存在属性，属性的操作和字典相同可以直接使用attrs获取全部属性tag的属性可以添加删除或者修改。操作和字典一致。 多值属性 一个属性可能有多个值，这里返回其列表。&lt;p class=&quot;body strikeout&quot;&gt;&lt;/p&gt;：将会返回body和strikeout的列表如果存在多个值，但是HTML定义中并没有定义为多值属性，那么还是返回字符串并且可以传入列表以修改多值属性，传入列表为多值。xml中不包含多值 可以遍历的字符串 字符串常被包含在tag内.Beautiful Soup用 NavigableString 类来包装tag中的字符串节点下的字符串不可以编辑，但是可以替换，使用replace_with(‘xxx’)字符串不支持：contents，string属性或find()方法需要在此之外使用字符串，使用unicode将其转换为普通的Unicode字符串，减少内存占用。 子节点 soup.body.b：body节点下的b节点 tag.contents：可以将节点下的子节点以列表方式输出。 tag.children：子节点生成器， descendants：后裔，多所有的子孙节点递归循环，这样会将string也单独提取出来 节点下的string：如果节点下面存在子节点，并且子节点不是内容节点，那么返回None strings and stripped_strings：解决了上面的问题，使用这个将打印节点下的所以字符串，其中stripped_strings去除空格 BeautifulSoup对象 表示文档的全部内容大部分时候可以将它当做Tag对象。 搜索文档树 很多搜索方法，这里解释：find()和find_all()其它操作方法类似 过滤器 字符串使用字符串来过滤查找，比如soup.find_all(&#39;a&#39;)来找到a标签 正则表达式支持正则表达式，比如：soup.find_all(re.compile(&quot;^b&quot;)) 列表参数传入列表参数：[&#39;a&#39;, &#39;b&#39;] 找出a b的标签 True传入True值，那么会找到所有节点，但是没有字符串。 方法如果没有合适的过滤器，那么可以传入方法，方法接收一个参数```def has_class_but_no_id(tag): return tag.has_attr(‘class’) and not tag.has_attr(‘id’) def not_lacie(href): return href and not re.compile(“lacie”).search(href)soup.find_all(href=not_lacie) from bs4 import NavigableStringdef surrounded_by_strings(tag): return (isinstance(tag.next_element, NavigableString) and isinstance(tag.previous_element, NavigableString)) &gt; 具体参数 find_all( name , attrs , recursive , string , **kwargs ) &gt; * name 用来查找所以tag &gt; * attrs 传入字典 &gt; * recursive 传入False只便利直接的子节点 &gt; * string 查找内容，页面上的文本内容，可以传入文字，列表，正则表达式，True &gt; * **kwargs 传入关键字参数 比如：id=&#39;xxx&#39; 同时可以过滤多个这样的属性 &gt; * limit 限制查找的数量 &gt; 有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性: data_soup = BeautifulSoup(‘foo!‘)data_soup.find_all(data-foo=”value”)SyntaxError: keyword can’t be an expression &gt; 但是可以通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的tag: data_soup.find_all(attrs={“data-foo”: “value”})[foo!] &gt; 但标识CSS类名的关键字 class 在Python中是保留字,使用 class 做参数会导致语法错误.从Beautiful Soup的4.1.1版本开始,可以通过 class_ 参数搜索有指定CSS类名的tag: ### 子节点 &gt; 一个节点可能包含一个或者多个子节点 &gt; * contents and children：返回子节点的列表，后者返回迭代器。 &gt; * descendants：后裔，会将字符串也打印出来。 &gt; * srting：得到节点下的NavigableString对象 &gt; * strings and stripped_strings：得到节点下的所以NString对象，后者会去除空格和空行。 ### 父节点 &gt; 每个节点或者字符串都有父节点 &gt; * parent：得到直接父节点 &gt; * parents：得到所有的父节点 ### 兄弟节点 &gt; * next_sipling and previous_sibling：类似遍历兄弟节点 &gt; * next_siplings and previous_siblings：当前节点之前或者之后的所有兄弟节点迭代输出 ### 回退和前进 &gt; * next_element and previous_element：返回当前节点上一个或者下一个被解析对象 &gt; * next_elements and previous_elements：对应的就是前后所有被解析的对象，就像HTML对象正在被解析一样 ### 像调用 find_all() 一样调用tag soup.find_all(“a”)soup(“a”) &gt; 这两行是等价的 ### find() &gt; find_all是得到所有的结果，有时候我们只需要一个结果，那么直接使用find就行。 &gt; find_all 加上参数limit = 1，也可以实现，但是返回的结果是列表 &gt; find没有找到结果的时候返回`None` &gt; find方法多次调用： html.find(‘head’).find(‘title’) ### find_parents() 和 find_parent() &gt; 搜索当前节点的父节点 s = html.find(string=’Lacie’) # 找到字符串Lacies.find_parents(‘a’) # 字符串节点的父节点 &gt; 其实方法就是使用了parent和parents属性进行迭代搜索的 ### find_next_siblings() 和 find_next_sibling() &gt; 通过next_siblings属性对当前tag之后的兄弟节点进行迭代搜索 first_link = html.afirst_linkElsiefirst_link.find_next_siblings()[Lacie, Tillie]``` find_previous_siblings() 和 find_previous_sibling() 和上面类似 find_all_next() 和 find_next() 通过next_elements属性对当前tag进行迭代搜索 &gt;&gt; f_link = html.a &gt;&gt; f_link &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt; &gt;&gt; f_link.find_all_next(&#39;a&#39;) [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a clas s=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;] &gt;&gt; f_link.find_all_next(string=True) [&#39;Elsie&#39;, &#39;,\\n&#39;, &#39;Lacie&#39;, &#39; and\\n&#39;, &#39;Tillie&#39;, &#39;;\\nand they lived at the bottom o f a well.&#39;, &#39;\\n&#39;, &#39;...&#39;, &#39;\\n&#39;] find_all_previous() 和 find_previous() 通过previous_element对当前tag进行迭代搜索 &gt;&gt; f_link = html.a &gt;&gt; f_link &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt; &gt;&gt; f_link.find_all_previous(&#39;title&#39;) [&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;] CSS选择器 bs4支持大部分的CSS选择器：CSS Selector w3school 在 Tag 或 BeautifulSoup 对象的 .select() 方法中传入字符串参数, 即可使用CSS选择器的语法找到tag: soup.select(&quot;title&quot;) # [&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;] soup.select(&quot;p nth-of-type(3)&quot;) # [&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;] 不是很懂这个选择器，大概看起来可以根据CSS选择器文档来书写。 修改文档树修改tag的名称和属性 tag.name = &#39;blockquote&#39;tag[&#39;class&#39;] = &#39;modify value&#39;tag[&#39;id&#39;] = &#39;newid&#39; 修改 .stringtag.string = &#39;New link text&#39; append()在.string上面追加内容 NavigableString() 和 .new_tag()NavigableString() 是构造一个字符串，然后将其放入需要修改的地方。new_string = NavigableString(&#39;Hello&#39;)html.b.append(new_string)from bs4 import Commentnew_comment = html.new_string(&#39;Nice to see you&#39;, Comment)html.b.append(new_comment)创建一个Tag的最好的方法是调用工厂方法，new_tag()new_tag = html.new_tag(&#39;a&#39;, href=&#39;test&#39;)html.b.append(new_tag) # 在b标签的下面追加一个tag a insert()选择插入的位置，这个和append类似，不过可以选择位置插入。 insert_before, insert_after看名字就知道，这是在当前tag或者string后边插入对象。定位到tag或者string，然后使用方法进行插入。 ‘clear()’移除tag下的内容 ‘extract()’移除当前tag，将tag移除。该方法移除tag的时候，会返回移除的tag信息 &gt;&gt;&gt; html = &#39;&lt;h1&gt;&lt;a href=&quot;www.text.com&quot;&gt;wwwww&lt;/a&gt;&lt;/h1&gt;&#39; &gt;&gt;&gt; h = bs(html, &#39;lxml&#39;) &gt;&gt;&gt; h &lt;html&gt;&lt;body&gt;&lt;h1&gt;&lt;a href=&quot;www.text.com&quot;&gt;wwwww&lt;/a&gt;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; &gt;&gt;&gt; h.body.a &lt;a href=&quot;www.text.com&quot;&gt;wwwww&lt;/a&gt; &gt;&gt;&gt; tag = h.body.a.extract() &gt;&gt;&gt; tag &lt;a href=&quot;www.text.com&quot;&gt;wwwww&lt;/a&gt; &gt;&gt;&gt; h &lt;html&gt;&lt;body&gt;&lt;h1&gt;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; &gt;&gt;&gt; ‘decompose()’ 这个更为激进，移除tag之后，不返回。 ‘replace_with()’移除内容并替换新的tag或者stringreturn：方法返回被替换的内容 ‘wrap()’对元素进行包装，并且返回包装后的内容。 &gt;&gt; tag = &lt;a&gt;xxx&lt;/a&gt; &gt;&gt; wrap_tag = BeautifulSoup.new_tag(&#39;p&#39;) &gt;&gt; tag.wrap(wrap_tag) &gt;&gt; tag &lt;p&gt;&lt;a&gt;xxx&lt;/a&gt;&lt;/p&gt; ‘unwrap()’ 和wrap的方法相反```markup = ‘I linked to example.com‘soup = BeautifulSoup(markup)a_tag = soup.a a_tag.i.unwrap()a_tag I linked to example.com#### 格式化输出 &gt; prettify() : 格式化为unicode之后，每个xml、html独占一行。 html.prettify()‘\\n \\n \\n I linked to\\n \\nexample.com\\n \\n \\n \\n‘print(html.prettify()) I linked to example.com ``` 压缩输出 unicode(bs)、str(bs)和encode这应该属于其它操作了 输出格式bs会将html里边的特殊字符转换成unicode的格式 &gt;&gt; soup = bs(&quot;&amp;ldquo;Dammit!&amp;rdquo; he said.&quot;) unicode(soup) # u&#39;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;\\u201cDammit!\\u201d he said.&lt;/body&gt;&lt;/html&gt;&#39; python3上面没有unicode，直接使用str替换。并且3上面bs4是直接将&amp;再次转义了，其它没有变化。 ‘get_text()’如果想要获取tag里边的所以文本内容，可以使用这个方法。‘get_text(‘|’) # 第一个参数指定分隔符号，不太明白这个分隔符具体操作，单词？？？’‘get_text(‘|’, trip=True) # 这个会处理文本两端的空格’还可以使用stripped_string生成器，获取文本列表分别处理。 指定解析器 ‘BeautifulSoup()’ 第一个参数是文本或者是文件，第二个参数是解析器，Python3应该不自动选择解析器。目前支持解析：html、xml和html5解析器有：lxml、html5lib和html.parser 解析器之间的区别使用了不同的解析器会产生不同的结果具体还是去官网查看API，这里就这样了。存在一些加标签和保留特定的标签的差异：比如lxml会将单结束标志’‘去掉，而html5lib会将其补齐为’‘一般不全的都会补齐基本标签，比如：’‘官方给的建议是在代码上面标注使用了什么解析器。 编码 任何文档都有其编码方式，使用bs4之后都将其解析为unicodebs使用了自动编码检测来将文档的编码转换为unicode‘original_encoding’属性记录了自动识别的编码bs采用了逐字节的方式来识别编码的，存在效率问题，并且还会猜错。所以可以使用from_encoding参数来指定文档的编码：BeautifulSoup(xxx, ‘lxml’, from_encoding=’utf-8’)当编码比较接近或者是其子集的时候，可能会猜测错误，并且我们也只是知道大致编码的时候，这个时候bs提供exclude_encoding参数来排除错误的编码。相当于猜测这个文档编码的一个集合，然后取其最可能的编码方式，但是如果猜测错误，那么说明猜测的这个编码是最可能的，但是是错误的，所以提供这个方法来排除，然后相当于让其猜测第二个可能的编码方式。例如：’BeautifulSoup(xxx, ‘lxml’, exclude_encoding=[‘utf-8’])’原文：少数情况下(通常是UTF-8编码的文档中包含了其它编码格式的文件),想获得正确的Unicode编码就不得不将文档中少数特殊编码字符替换成特殊Unicode编码,“REPLACEMENT CHARACTER” (U+FFFD, �) [9] . 如果Beautifu Soup猜测文档编码时作了特殊字符的替换,那么Beautiful Soup会把 UnicodeDammit 或 BeautifulSoup 对象的 .contains_replacement_characters 属性标记为 True .这样就可以知道当前文档进行Unicode编码后丢失了一部分特殊内容字符.如果文档中包含�而 .contains_replacement_characters 属性是 False ,则表示�就是文档中原来的字符,不是转码失败. 输出编码 bs将输入的文档都转换为utf-8输出，并且会将源数据的meta里边的编码也改成utf-8在’prettify(‘utf-8’)’可以指定打印其编码并且调研bs对象或者节点的方式来使用encode()方法‘tag.encode(‘utf-8’)’如果文档中包含当前编码不支持的字符，那么会进行转码。 &gt;&gt; markup = u&quot;&lt;b&gt;\\N{SNOWMAN}&lt;/b&gt;&quot; &gt;&gt; snowman_soup = BeautifulSoup(markup) &gt;&gt; tag = snowman_soup.b &gt;&gt; print(tag.encode(&quot;utf-8&quot;)) # &lt;b&gt;☃&lt;/b&gt; &gt;&gt; print tag.encode(&quot;latin-1&quot;) # &lt;b&gt;&amp;#9731;&lt;/b&gt; &gt;&gt; print tag.encode(&quot;ascii&quot;) # &lt;b&gt;&amp;#9731;&lt;/b&gt; ‘UnicodeDammit’ 这个是bs的内置库，可以用来猜测文档的编码。UnicodeDammit(‘xxx’).unicode_markup.original_encoding就可以查看猜测的编码了。UnicodeDammit.detwingle()：可以将编码进行统一，比如一些网站包含了另外的网站内容，但是编码却不一样，一个是utf-8编码，一个是另外的编码这样直接使用是存在问题的。所以在处理之前最好使用这个方法进行操作一次。 判断对象是否相同 使用等于不严格的方式来判断是否相同，如下图不同位置b对象是相同的 &gt;&gt; markup = &quot;&lt;p&gt;I want &lt;b&gt;pizza&lt;/b&gt; and more &lt;b&gt;pizza&lt;/b&gt;!&lt;/p&gt;&quot; &gt;&gt; soup = BeautifulSoup(markup, &#39;html.parser&#39;) &gt;&gt; first_b, second_b = soup.find_all(&#39;b&#39;) &gt;&gt; print(first_b == second_b) # True &gt;&gt; print(first_b.previous_element == second_b.previous_element) # False 使用is来严格判断是否相同 &gt;&gt; print(first_b is second_b) False 复制Beautiful Soup对象 copy.copy()可以复制任意的Tag或NavigableString import copy p_copy = copy.copy(soup.p) 解析部分文档如果仅仅需要获取页面中的a标签的内容，但是如果全部都去解析，就太耗费性能和内存了。最快的方法就在一开始就把a标签以外的都忽略掉。SoupStrainer类可以定义文档的某段内容，只会解析SoupStrainer中定义的内容，创建一个SoupStrainer对象并作为parse_only参数给BeautifulSoup的构造方法就行。SoupSTrainer类接受与典型搜索方法相同的参数：name、attr、recursive、string、**kw，下面源教程的列子，最后的short_strings运行错误，可能是参数导致的，显示NoneType无len。```from bs4 import SoupStrainer only_a_tags = SoupStrainer(“a”) # only_tags_with_id_link2 = SoupStrainer(id=”link2”) def is_short_string(string): return len(string) &lt; 10 only_short_strings = SoupStrainer(string=is_short_string) BeautifulSoup(html_doc, “html.parser”, parse_only=only_a_tags)``` 其他说明（官方教程为代码诊断） from bs4.diagnose import diagnose这个可以打印出解析器处理文档的过程。使用了一下，貌似是使用了不同的解析器对文档进行解析，然后分别给出结果。 解析错误解析崩溃或者异常基本都不是bs4的问题，因为他并不包含解析器，官方给于的解释是换解析器最常见的解析错误是HTMLParser.HTMLParseError: malformed start tag和HTMLParser.HTMLParseError: bad end tag.这都是由Python内置的解析器引起的,解决方法是 安装lxml或html5libfind_all()或者find返回都是空，但是文档中也存在这个Tag，还是换解析器。 解析器错误一般解析器解析之后都是转为小写，如果要保留大小写，那么使用xml解析器 效率问题使用lxml解析器会较快一点安装cchardet之后，文档编码检测速度会更快。 完","categories":[{"name":"Python","slug":"Python","permalink":"https://returnwow.github.io/categories/Python/"}],"tags":[{"name":"学习 python bs4","slug":"学习-python-bs4","permalink":"https://returnwow.github.io/tags/学习-python-bs4/"}]},{"title":"Numpy基础","slug":"python/numpy/02_quickstart_tutorial","date":"2017-11-20T13:51:00.934Z","updated":"2017-11-28T14:31:44.030Z","comments":true,"path":"2017/11/20/python/numpy/02_quickstart_tutorial/","link":"","permalink":"https://returnwow.github.io/2017/11/20/python/numpy/02_quickstart_tutorial/","excerpt":"","text":"NumPy基础 NumPy的主要对象是同类型的多维数组，是一张表，所有元素（通常是数字）的类型都相同。维度称为axes，axes的数目为rank。如下rank为2（2维的），第一维度axes长度为2，第二维度axes长度为3.[[ 1., 0., 0.],[ 0., 1., 2.]]NumPy的数组的类称为ndarray。Python的数组提供较少的功能，而ndarray一下重要的属性。 属性 ndarray.ndim：数组的axes（维度）的个数，维度的数量称为rank。 ndarray.shape：数组的维度。这是一个整数元组，(n, m)表示一个n行，m列的矩阵。 ndarray.size：为shape元素的乘积，其实就是矩阵元素个数。 ndarray.dtype：描述矩阵中元素的类型的对象。可以使用Python标准的类型，也提供其他类型，例如：numpy.int32、numpy.int16和numpy.float64。 ndarray.itemsize：数组中每个元素的字节大小。例如float64字节数为8。 ndarray.data：该缓冲区包含数组的实际元素。通常不适用这个，我们一般使用索引进行访问（前面矢量化又说不要遍历，后续看看是怎么回事）。 打开中文API查看示例：属性示例 数组创建 有几种方法来创建数组 可以使用array函数从常规的Python列表或元组中创建数组，得到的数组类型从序列中元素的类型推到而出。&gt;&gt;&gt; numpy.array([3, 4, 5]) array函数传入列表的列表产生2维阵列，同理3维。&gt;&gt;&gt; numpy.array([1, 2, 3], [4, 5, 6])array([[1, 2, 3][4, 5, 6]]) 数组的类型可以在创建的时候指定&gt;&gt;&gt; numpy.array([1, 2, 3], dtype=complex) #元素为复数形式 1. + 0.j 通常数组的元素是位置的，所有NumPy提供创建具体初始占位符的数组，减少数组增长的必要。 函数zeros创建一个由0组成的数组； 函数ones创建一个由1数组的数组； 函数empty内容是随机的并且取决于存储器的状态，元素默认类型是float64。 为了创建数组序列，NumPy提供了类似于range的函数，返回数组而不是列表。 &gt;&gt;&gt; numpy.arange(1, 10, 2) #使用如上的reshape函数可以改变形状。起始为1步长为2。 当arange与浮点参数一起使用时，由于浮点数的精度是有限的，通常不可能预测获得的元素数量。出于这个原因，通常最好使用函数linspace，它接收我们想要的元素数量而不是步长作为参数：&gt;&gt;&gt; numpy.linspace(1, 2, 9) # 1到二之间的9个数，感觉是平均分配的样子。步长0.125‘&gt;&gt;&gt; numpy.sin(numpy.linspace(0, 2*pi, 100))’ #求sin。其他函数：array, zeros, zeros_like, ones, ones_like, empty, empty_like, arange,linspace, numpy.random.rand, numpy.random.randn, fromfunction, fromfile 打印数组 当打印数组时，NumPy以类似于嵌套列表的方式显示它，但是使用以下布局： 最后一个axes从左到右打印， 第二个到最后一个从上到下打印， 其余的也从上到下打印，每个切片与下一个用空行分开 如果数组太大，会跳过中间的部分，只打印边角部分。 如果需要强制打印整个数组，可以使用set_printoptions来更改打印选项numpy.set_printoptions(threshold=&#39;nan&#39;) #官方api说传入的是int值，nan目前不知道什么意思：后面查看可以传入numpy.nan，而非str类型 reshape改变数组形状 a.ravel() #平坦化序列，比如3 * 4的矩阵，变成一维矩阵。 a.reshape(x, y) #转变形状到x*y 元素需要支持转换该形状才行，如果y=-1则会自动计算维度。 a.T #转置 transposed numpy.resize方法直接改变数组本身 基本操作数组上的算术运算符使用元素基本，将创建一个新数组并用结果填充，差不多就是对应到每个元素，然后返回计算结果。 &gt;&gt;&gt; a = numpy.array([20, 30, 40, 50])&gt;&gt;&gt; b = numpy.arange(4) &gt;&gt;&gt; c = a - b #对应求其差值 &gt;&gt;&gt; b**2 #b元素都乘2 &gt;&gt;&gt; 10*np.sin(a) #对a每个元素求sin，并乘10倍值。 &gt;&gt;&gt; a &lt; 35 #对a元素做判断，返回boolean矩阵。 dot函数做矩阵的乘法：a.dot(b) np.dot(a, b) a * b 是对a和b对应位置的元素做乘积 += *= 类似的操作会修改当前数组，而不是创建新的数组： ‘&gt;&gt;&gt; a = np.ones((2,3), dtype=int)’ &gt;&gt;&gt; b = np.random.random((2, 3)) &gt;&gt;&gt; a *= 3 &gt;&gt;&gt; b += a &gt;&gt;&gt; a += b #报错， float64 to int32 不同类型的数组操作，结果数组的类型对应更精确的数组，向上转型。 比如int32 和 float64计算 结果类型为float64 许多一元操作，求和sum，可以使用ndarray的方法 sum(), max(), min() 默认的这些操作适用于数组，就像一个数字列表求最大值，不管其形状（shape = (2*3)）通过axis参数，可以指定沿数组的指定轴应用操作 b.sum(axis=0) # 每列求和 b.sum(axis=1) # 每行求和 b.cumsum(axis=1) # 沿着行累积和，相反沿着列累积和 另见 all, any, apply_along_axis, argmax, argmin, argsort, average, bincount, ceil, clip, conj, corrcoef, cov, cross, cumprod, cumsum, diff, dot, floor, inner, inv, lexsort, max, maximum, mean, median, min, minimum, nonzero, outer, prod, re, round, sort, std, sum, trace, transpose, var, vdot, vectorize, where all：是判断所以元素是否都为True，可以定义axis来沿着某个轴判断。 any：和all相反，判断是否有一个为Ture，是返回True，否返回False，可以传入axis 例如：b.any(0)，纵轴是否有为真的。 apply_along_axis(func, axis, arr)：将arr按照axis定义，行或者列取出作用在func上，并返回一个列表。 如果上述返回标量，则返回于源arr相同形状(shape)。比如传输sorted进行排序，返回还是列表。 argmax：查找最大值，返回索引，支持axis轴选择。如果多个大值，就返回第一次遇到的。 argmin：和max相反。 argsort：排序，返回排序完的索引值。可以选择算法。选择键排序。 mean：平均值，可以按轴进行。axis diff：计算差分，out[n] = a[n+1] - a[n]，可以按轴进行。 vdot：计算点积。 还有很多函数，先不看了。 索引、切片和迭代一维数组支持索引，切片和迭代，非常类似于列表和其他序列。 a[1:2]，其中**是次方，**(1/3) 求根号3 多维数组每个轴可以有一个索引。这些索引以逗号分隔的元组给出： 比如b[2,3]表示3行4列的元素，b[0:3, 1]表示行数是0,1,2的第2列，如果1数字缺失，则认为是全部列。如果维度大于3，比如维度（轴，rank）为5的情况，x[1,2,:,:,:]可以等效为x[1,2,…] x[1,2,…]等效于x[1,2,:,:,:] x[…,3]到x[:,:,:,:,3] x[4,…,5,:]到x[4,:,:,5,:]例子：y[1:5:2,::3]指的是1-4行间隔为2行，列数间隔为3进行抽取。避免迭代使用索引提高性能。可以使用一个数组来索引数组，提高的索引必须是索引值，比如a[np.array([1,2,3])] 索引a中的索引为1,2,3的元素。并且索引数组可以是多维的生成的矩阵维度和索引一致，a[np.array([[1,2], [3,4]])]，二维矩阵，1,2,3,4对应为a一维矩阵的索引。 Boolean型索引，比如&gt;&gt;&gt; y = np.arange(35).reshape(5,7)&gt;&gt;&gt; b = y &gt; 22&gt;&gt;&gt; y[b] # 这样就将y里边的大于22的数索引出来了，但是维度只有一维，并且如果索引的布尔数组和y具有相同的形状通常，当布尔数组具有比被索引的数组更少的维度时，这等同于y [b，…]，这意味着y被索引为b，然后是多个：如同填充y。因此，结果的形状是包含布尔数组的True元素的数目的一个维度，后面是被索引的数组的剩余维度。组合索引和切片，感觉就是行和列的索引，三维的话相当于两个数组，就分别对两个进行索引 y[1:3, 2:3] #类似的就是1-2行并且列是2列结构化索引工具&gt;&gt;&gt; y.shape(5, 7)&gt;&gt;&gt; y[:,np.newaxis,:].shape # np.newaxis 对象来新建一个维度&#39;(5, 1, 7)` 可以使用索引来改变相应位置的值y[2:7] = 1 #将2-6位置的数更改为1y[2:7] = np.arange(5) #或者直接使用数组的形式如果高类型分配给低类型的，会转型，损失精度，或者抛出错误下面的一个例子比较特殊：&gt;&gt;&gt; x = np.arange(0, 50, 10)&gt;&gt;&gt; xarray([ 0, 10, 20, 30, 40])&gt;&gt;&gt; x[np.array([1, 1, 3, 1])] += 1&gt;&gt;&gt; xarray([ 0, 11, 20, 31, 40])实际上我们认为1位置的数应该增加3才对，但是最终只增加1，其实是我们每次计算都采用了临时数组的方式，导致最终只增加1. Ellipsis 等同于 ‘…’ 元组不像列表索引那样会自动处理，如下例子：&gt;&gt;&gt; z[[1,1,1,1]] # produces a large arrayarray([[[[27, 28, 29],[30, 31, 32], ...&gt;&gt;&gt; z[(1,1,1,1)] # returns a single value40 多维数组迭代是相对于第一个轴进行的。 可以使用flat，是一个迭代器。如果使用x[0][2]会降低性能。因为2索引需要在前面0索引创建的数组上面继续操作。另见indexing。 形状操作 以下的命令不会修改数组，只会返回新的数组 ravel 返回连续的平坦的数组，降成一维 reshape 修改形状 T 转置修改数组本身 resize：对应的reshape是返回修改的数组将不同的数组堆叠 vstack hstack h v 分别对应水平方向和垂直方向。 column_stack row_stack column 和 row 分别对应，并且这两个方法允许1D的数组堆叠到2D数组中将数组分隔成几个小的数组 np.hsplit(a, 3) #将a划分为3个 h是水平 np.hsplit(a, (3, 4)) #将列为3-4但是不等于4的索引分隔开来 同理vsplit是垂直划分 array_split 可以指定某个轴进行划分 复制和视图 当计算和操作数组时，它们的数据有时被复制到新的数组中，有时不复制。这通常是初学者的混乱的来源。有三种情况： 完全不复制简单赋值不会创建数组对象或其数据的拷贝：如果将b = a 那么b和a指向的是同一个数组，改变b，a也跟着改变。可变对象传递到函数引用，函数不会复制。 视图或浅复制不同的数组对象可以共享相同的数据。view方法创建一个新数组对象，该对象看到相同的数据。c=a.view(), 这样改变c.shape = (2,6)，a的形状不会改变，但是如果c[0,4] = 1234，a的相应位置就会赋值为1234c.base is a = True对数组切片返回的也是视图 深复制d = a.copy()这样d就是一个新的数组 函数和方法概述 这里是一些有用的NumPy函数和方法名称按类别排序的列表。有关完整列表，请参见Routines。 数组创建arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r, zeros, zeros_like 转换ndarray.astype，atleast_1d，atleast_2d，atleast_3d，mat 操纵array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack 问题all，any，nonzero，where 顺序argmax, argmin, argsort, max, min, ptp, searchsorted, sort 操作choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask, real, sum 基本统计cov，mean，std，var基本线性代数cross，dot，outer，linalg.svd，vdot 其他基础广播规则 Broadcasting允许通用函数以有意义的方式处理具有不完全相同形状的输入。 Broadcasting的第一个规则是，如果所有输入数组不具有相同数量的维度，则“1”将被重复地添加到较小数组的形状，直到所有数组具有相同数量的维度。 Broadcasting的第二个规则确保沿着特定维度具有大小为1的数组表现得好像它们具有沿着该维度具有最大形状的数组的大小。假定数组元素的值沿“Broadcasting”数组的该维度相同。 花式索引技巧 NumPy提供了比常规Python序列更多的索引能力。除了通过整数和切片索引之外，如前所述，数组可以由整数数组和布尔数组索引。 使用索引数组索引a[j] j = np.array([1,1,3]) #得出索引a是1,1,3组成的数组如果索引的数组是多维的，结果与之对应，j = np.array([[1,2,3], [4, 5, 6]])，结果是形状是(2, 3) 可以对每个维度进行单独索引i = np.array([[0,1], [1, 2]])j = np.array([[2,1], [1, 3]])a[i, j]同样的 l=[i,j] a[l]=a[i,j] 函数 np.ix_[a,b,c] ???雨里雾里np.ufunc.reduce ???云里雾里 线性代数 基础 简单的数组操作 a.transpose() # 矩阵转置 np.linalg.inv(a) # 逆矩阵 np.eye(2) # 2*2的矩阵，返回对角线是1其他都是0的矩阵 np.dot(i, j) # 求矩阵乘积 np.trace(a) # 对a矩阵的对角线值求和 np.linalg.solve(a, y) # 解线性方程组 a矩阵对应参数，y对应等于后边的数 np.linalg.eig(j) # 特征向量，线性代数当时没有学好","categories":[{"name":"Python","slug":"Python","permalink":"https://returnwow.github.io/categories/Python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"https://returnwow.github.io/tags/Numpy/"}]},{"title":"What is Numpy","slug":"python/numpy/01_what_is_numpy","date":"2017-11-16T12:00:30.500Z","updated":"2018-01-27T01:55:36.412Z","comments":true,"path":"2017/11/16/python/numpy/01_what_is_numpy/","link":"","permalink":"https://returnwow.github.io/2017/11/16/python/numpy/01_what_is_numpy/","excerpt":"","text":"What is Numpy Numpy是Python科学计算的基本包，提供对于多维数组（矩阵）等各种计算，比如c = a * b，a、b均为矩阵，可以对其进行直接计算，不用使用循环遍历。本机测试计算速度为for循环遍历计算的500+倍。例子如下：目前找不到源网页，直接粘上例子。 import numpy as np import time #from timeit import timeit a = np.random.rand(1000000) b = np.random.rand(1000000) time_s = time.time() c = np.dot(a, b) time_e = time.time() print(c, 1000*(time_e - time_s)) c = 0 time_s = time.time() for i in range(len(a)): c += a[i] * b[i] time_e = time.time() print(c, 1000*(time_e - time_s)) 下图所示 矢量化(Vectorization) 上面的例子是矢量化的例子（Vectorization），包含以下优点： 代码更简洁 更少的代码通常意味着更少的错误 代码运行更接近标准的数学符号，封装了具体的困难的运算，如上np.dot(a, b)直接计算两个矩阵的积。 矢量化导致更多的“Pythonic”(网络解释为很Python的Python代码)代码。如果没有向量化，我们的代码将会效率很低，难以读取for循环。 广播(Broadcasting) 广播是用于描述操作的隐式逐个元素行为的术语大概意思就是比如做矩阵的运算，a为一维矩阵，b为标量，如果做计算a + b，会自动的将a的元素都加上b（我的简单理解） 源网站 Numpy 中文 API","categories":[{"name":"Python","slug":"Python","permalink":"https://returnwow.github.io/categories/Python/"}],"tags":[{"name":"Learn Numpy","slug":"Learn-Numpy","permalink":"https://returnwow.github.io/tags/Learn-Numpy/"}]},{"title":"Git删除提交的文件","slug":"git/git_rm_use_cached","date":"2017-11-05T12:26:57.179Z","updated":"2018-01-27T06:06:24.730Z","comments":true,"path":"2017/11/05/git/git_rm_use_cached/","link":"","permalink":"https://returnwow.github.io/2017/11/05/git/git_rm_use_cached/","excerpt":"","text":"目的为了测试Markdown书写 本人在学习 git 的时候，前期将本地的日志文件提交到仓库，然后也就查找了相关的操作方法，最终使用此命令解决的，然后新增.gitignore文件，提交，再次使用git status这些文件就不会再次出现了，其他情况也可以使用这个方法：将数据库账号密码配置文件从远端仓库删除。 git rmgit rm Filename or Foldername： 使用在当我们提交了一个不想提交的文件，可以使用其删除本地工作目录的文件，一般我们可能需要保留源文件，所以可以使用：--cached参数来实现只删除索引（官方：Index，跟踪的文件清单）。 git rm –cached Filename or Foldername： 使用 --cached 将会保留本地文件，只删除暂存区的文件，提交这次修改，该文件就不会纳入版本管理了，远端的仓库也会同步这个修改，达到删除文件目的。 常用参数： -n or --dry-run 加上这个参数是不会做任何删除操作的，只是将符合filename删除的文件预览打印出来，准确的说加上这个参数会打印此操作产生影响（删除）的文件列表。 git rm -n --cached txt/\\\\*.txt #将会列出txt目录下的符合拓展名字为.txt的文件，反斜杠\\为转义字符。 -f 这个参数是强制执行的意思，如果我们已经将文件提交到暂存区，这个时候需要加上这个参数。 -r 类似bash命令就是递归删除了，如果删除目录使用此参数。 官网相关页面： git-rm","categories":[{"name":"git","slug":"git","permalink":"https://returnwow.github.io/categories/git/"}],"tags":[{"name":"git rm 删除 文件","slug":"git-rm-删除-文件","permalink":"https://returnwow.github.io/tags/git-rm-删除-文件/"}]}]}